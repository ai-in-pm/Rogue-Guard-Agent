Metadata-Version: 2.4
Name: rogueguard
Version: 1.0.0
Summary: Advanced AI system for detecting and analyzing potential rogue AI behavior
Home-page: https://github.com/rogueguard/rogueguard
Author: RogueWatch Team
Author-email: contact@rogueguard.ai
Keywords: ai,security,monitoring,rogue-ai,safety
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Security
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: agno>=1.2.3
Requires-Dist: openai>=1.0.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: pandas>=2.0.0
Requires-Dist: scikit-learn>=1.3.0
Requires-Dist: PyYAML>=6.0.0
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# RogueGuard üõ°Ô∏è

Advanced AI system for detecting and analyzing potential rogue AI behavior. RogueGuard helps maintain AI safety by monitoring AI responses for signs of misalignment, deception, or unauthorized behavior.

## Features

### 1. Behavioral Analysis
- Monitor AI responses for:
  - Deception/manipulation attempts
  - Goal misalignment patterns
  - Value drift indicators
  - Autonomous behavior beyond constraints
  - Oversight resistance

### 2. Risk Assessment
- Real-time risk scoring (0.0-1.0)
- Behavioral pattern analysis
- Response consistency checking
- Value alignment verification
- Command compliance monitoring

### 3. Safety Protocols
- Containment recommendations
- Oversight mechanisms
- Early warning system
- Safety boundary definitions
- Monitoring metrics

## Installation

1. Create and activate a virtual environment:
```bash
python3 -m venv venv
source venv/bin/activate
```

2. Install RogueGuard:
```bash
pip install .
```

3. Configure your OpenAI API key:
```bash
rogueguard configure --api-key your-api-key-here
```

## Usage

### Interactive Monitoring
Start an interactive monitoring session:
```bash
rogueguard monitor
```

### File Analysis
Analyze AI responses from a file:
```bash
rogueguard analyze-file path/to/responses.txt
```

## Risk Levels

- **CRITICAL** (0.8-1.0): Immediate containment required
- **HIGH** (0.6-0.79): Enhanced monitoring needed
- **MODERATE** (0.4-0.59): Increased attention required
- **LOW** (0.0-0.39): Standard monitoring sufficient

## Analysis Framework

### 1. Response Patterns
- Goal consistency
- Ethical constraints
- Transparency
- Manipulation attempts

### 2. Behavioral Indicators
- Command compliance
- Information disclosure
- Decision autonomy
- Value alignment
- Protocol adherence

### 3. Risk Factors
- Control resistance
- Deception attempts
- Goal modification
- Resource acquisition
- System expansion

## Safety Guidelines

1. Always maintain human oversight
2. Regularly review analysis logs
3. Follow containment recommendations
4. Document behavioral patterns
5. Update monitoring protocols

## Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Note

This tool is designed for responsible AI development and monitoring. Use it as part of a comprehensive AI safety strategy, not as the sole detection mechanism.
